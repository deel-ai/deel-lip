{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86557797-36de-44bb-b277-2ee7a22b1458",
   "metadata": {},
   "source": [
    "# ðŸ‘‹ Getting started 1: Creating a 1-Lipschitz neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0071a-2546-4143-af7e-496d9ffa2fd4",
   "metadata": {},
   "source": [
    "The goal of this series of tutorials is to show the different usages of `deel-lip`.\n",
    "\n",
    "In this first notebook, our objective is to show how to create 1-Lipschitz neural networks with `deel-lip`. \n",
    "\n",
    "In particular, we will cover the following: \n",
    "1. [ðŸ“š Theoretical background](#theoretical_background)    \n",
    "A brief theoretical background on Lipschitz continuous functions. This section can be safely skipped if one is not interested in the theory.\n",
    "2. [ðŸ§± Creating a 1-Lipschitz neural network with `deel-lip` and `keras`](#deel_keras)       \n",
    "An example of how to create a 1-Lipschitz neural network with `deel-lip` and `keras`.\n",
    "3. [ðŸ”¨ Design rules for 1-Lipschitz neural networks with `deel-lip`](#design)   \n",
    "A set of neural network design rules that one must respect in order to enforce the 1-Lipschitz constraint.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ“š Theoretical background <a id='theoretical_background'></a> <a name='theoretical_background'></a>\n",
    "### What is a Lipschitz constant\n",
    "The `deel-lip` package allows to control the Lipschitz constant of a layer or of a whole neural network. The Lipschitz constant is a mathematical property of a function (in our context of work, a layer or a model) that characterizes how much the output of the function can change with respect to changes in its input. \n",
    "\n",
    "In mathematical terms, a function $f$ is Lipschitz continuous with a **Lipschitz constant L** or more simply **L-Lipschitz** if for any given pair of points $x_1,x_2$, $L$ provides a bound on the rate of change of $f$:  \n",
    "\n",
    "$$||f(x_1)-f(x_2)||\\leq L||x_1-x_2||.$$\n",
    "\n",
    "For instance, given a 1-Lipschitz dense layer (a.k.a fully connected layer) with a weight matrix $W$ and a bias vector $b$, we have for any two inputs $x_1$ and $x_2$: $$||W.x_1+b-(W.x_2+b)|| \\leq 1||x_1-x_2||.$$\n",
    "\n",
    "ðŸ’¡ The norm we refer to throughout our notebooks is the Euclidean norm (L2). This is because `deel-lip` operates with this norm. You will find more information about the role of the norm in the context of adversarially robust 1-Lipschitz deep learning models in the notebook titled 'Getting Started 2'.\n",
    "\n",
    "### A simple requirement for creating 1-Lipschitz neural network\n",
    "The composition property of Lipschitz continuous functions states that if you have a function f that is $L_1$-Lipschitz and another function g that is $L_2$-Lispchitz, then their composition function h = (f o g) which applies f after g is also Lipschitz continuous with a Lipschitz constant $L \\leq L_1$ * $L_2$.\n",
    "\n",
    "A feed-forward or sequential neural network is essentially a stack of layers, where each layer transforms the output of the previous layer(s) and feeds its output to the next ones. \n",
    "\n",
    "By the composition property of Lipschitz functions, *it suffices for each of the n individual layers of a neural network model to be 1-Lipschitz, for the whole model to be 1-Lipschitz*.\n",
    "\n",
    "For instance, given a 1-Lipschitz dense layer parametrized by $(W_1,b_1)$, and a ReLU (Rectified Linear Unit) activation layer which is naturally 1-Lipschitz, the combination of the two is also 1-Lispchitz.   \n",
    "This is shown in the equations below, where we have for any two inputs $x_1$ and $x_2$:\n",
    "\n",
    "$$||W_1.x_1+b_1-(W_1.x_2+b_1)||\\leq 1||x_1-x_2||,$$\n",
    "$$||ReLU(x_1)-ReLU(x_2)||\\leq 1||x_1-x_2||,$$\n",
    "and:\n",
    "$$||ReLU(W_1.x_1+b_1)-ReLU(W_1.x_2+b_1)||\\leq 1||W_1.x_1+b_1-(W_1.x_2+b_1)||\\leq 1^2||x_1-x_2||.$$\n",
    "\n",
    "\n",
    "The `deel-lip` package allows to create 1-Lipschitz neural networks, by providing the user with means to enforce the Lipschitz constant at one on a selected set of layers (such as dense layers). \n",
    "It also ensures that 1-Lipschitz continuity is retained during training.\n",
    "\n",
    "\n",
    "## ðŸ§± Creating a 1-Lipschitz neural network with `deel-lip` and `keras` <a id='deel_keras'></a> <a name='deel_keras'></a>\n",
    "`keras` is an open-source high-level deep learning API written in Python. It allows to build, train, and deploy deep learning models.\n",
    "\n",
    "One can produce a neural network architecture using keras with a few lines of code, as shown in the toy-example multi-layer perceptron (MLP) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3f0694-8547-4d06-b2aa-bfc0d008ff8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52650 (205.66 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes=10\n",
    "\n",
    "# a basic model that does not follow any Lipschitz constraint\n",
    "model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(32),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "          loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b53d4-5585-41d9-a301-0012c54dba0b",
   "metadata": {},
   "source": [
    "Alternatively, it is equivalent to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f7099e-a425-452d-9ed5-5328f0258a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52650 (205.66 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dense(32)(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "y = layers.Dense(num_classes)(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f72425",
   "metadata": {},
   "source": [
    "`deel-lip` extends `keras`' capabilities by introducing custom `layers` and `model` modules, to provide the ability to control the Lipschitz constant of layers objects or of complete neural networks, while keeping a user-friendly interface.\n",
    "\n",
    "Below is a 1-Lipschitz replication of the previous MLP toy-example, using `deel-lip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62d3a3f-c0e8-4a3e-9025-758afebf99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deel\n",
    "from deel import lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26f52a6-c2ec-49b5-a99f-3353dc3f3044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kierszbaums\\anaconda.related\\envs\\1_lipschitz\\deel_lip\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " spectral_dense (SpectralDe  (None, 64)                100481    \n",
      " nse)                                                            \n",
      "                                                                 \n",
      " group_sort2 (GroupSort2)    (None, 64)                0         \n",
      "                                                                 \n",
      " spectral_dense_1 (Spectral  (None, 32)                4161      \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " group_sort2_1 (GroupSort2)  (None, 32)                0         \n",
      "                                                                 \n",
      " spectral_dense_2 (Spectral  (None, 10)                661       \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105303 (411.34 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 52653 (205.68 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "L1_model = lip.model.Sequential([    \n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Flatten(),\n",
    "        lip.layers.SpectralDense(64),\n",
    "        lip.layers.GroupSort2(),\n",
    "        lip.layers.SpectralDense(32),\n",
    "        lip.layers.GroupSort2(),\n",
    "        lip.layers.SpectralDense(num_classes)\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "L1_model.compile(optimizer='adam',\n",
    "          loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "L1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaddb4",
   "metadata": {},
   "source": [
    "Alternatively, it is equivalent to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a98e31f-40a9-46f5-a4e6-bd91229046e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " spectral_dense_3 (Spectral  (None, 64)                100481    \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " group_sort2_2 (GroupSort2)  (None, 64)                0         \n",
      "                                                                 \n",
      " spectral_dense_4 (Spectral  (None, 32)                4161      \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " group_sort2_3 (GroupSort2)  (None, 32)                0         \n",
      "                                                                 \n",
      " spectral_dense_5 (Spectral  (None, 10)                661       \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105303 (411.34 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 52653 (205.68 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "x = lip.layers.SpectralDense(64)(x)\n",
    "x = lip.layers.GroupSort2()(x)\n",
    "x = lip.layers.SpectralDense(32)(x)\n",
    "x = lip.layers.GroupSort2()(x)\n",
    "y = lip.layers.SpectralDense(num_classes)(x)\n",
    "L1_model = lip.model.Model(inputs=inputs, outputs=y)\n",
    "L1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8046c-115a-4fea-b36e-16b05a811897",
   "metadata": {},
   "source": [
    "ðŸ’¡\n",
    "Keep in mind that all the classes above inherit from their respective `keras` equivalent (e.g. `Dense` for `SpectralDense`).   <br>\n",
    "As a result, these objects conveniently use the same interface and the same parameters as their keras equivalent.\n",
    "\n",
    "## ðŸ”¨ Design rules for 1-Lipschitz neural networks with `deel-lip`  <a id='design'></a> <a name='design'></a>\n",
    "**Layer selection: `deel-lip` vs `keras`**  \n",
    "<br/> \n",
    "In our 1-Lipschitz MLP examples above, we have used a mixture of objects from both `keras` and `deel-lip` `layers` submodule (e.g. the `Input` layer for `keras`, the `SpectralDense` layer for `deel-lip`).\n",
    "\n",
    "More generally, for the particular types of layers that do not interfere with the Lipschitz property of any neural network they belong to, no alternative has been coded in `deel-lip` and the existing `keras` layer object can be used. \n",
    "\n",
    "This is the case for the following keras layers: `MaxPooling`, `GlobalMaxPooling`, `Flatten` and `Input`.\n",
    "\n",
    "Below is the full list of `keras` layers for which `deel-lip` provides a Lipschitz equivalent. If one wants to ensure a model's Lipschitz continuity, the alternative `deel-lip` layers must be employed instead of the original `keras` counterparts.\n",
    "\n",
    "| tensorflow.keras.layers | deel.lip.layers |\n",
    "| --------------- | --------------- |\n",
    "| `Dense`    | `SpectralDense`<br>|\n",
    "| `Conv2D`   | `SpectralConv2D`<br>  |\n",
    "|  `AveragePooling2D`<br>`GlobalAveragePooling2D` | `ScaledAveragePooling2D`<br>`ScaledGlobalAveragePooling2D`|\n",
    "\n",
    "<br/>\n",
    "\n",
    "ðŸ’¡ Although there are additional Lipschitz continuous layers available in `deel-lip`, the ones mentioned above are perfectly suitable and recommended for practical use. Interested readers can find information about the other layers [here](https://deel-ai.github.io/deel-lip/api/layers/).\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "ðŸš¨ **Note:** *When creating a 1-Lipschitz neural network, one should avoid using the following layers:*<br> \n",
    "- `Dropout`: Our current recommendation is to avoid using it, since it can induce a modification of the Lipschitz constant of the model.\n",
    "- `BatchNormalization`: It is not 1-Lipschitz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0f4f-16de-4172-a4e9-b4798f0bd678",
   "metadata": {},
   "source": [
    "\n",
    "**Activation function selection:**\n",
    "\n",
    "The ReLU activation function is Lipschitz continuous with a Lipschtiz constant of 1. \n",
    "\n",
    "However, the 'GroupSort2' activation function provided in the `layers` submodule of `deel-lip` has additional properties that can enhance the adversarial robustness of 1-Lipschitz neural networks.\n",
    "\n",
    "ðŸ’¡ Interested readers can find information relevant to other 1-Lipschitz activation functions that exist within `deel-lip` [here](https://deel-ai.github.io/deel-lip/api/layers/).\n",
    "\n",
    "\n",
    "**Loss function selection:**\n",
    "\n",
    "One can use `keras` loss functions to train 1-Lipschitz neural networks. Doing so will not interfere with the 1-Lipschitz continuity of the model.  \n",
    "\n",
    "ðŸ’¡ `deel-lip` also has a `losses` submodule that contains several loss functions. They have been developped to enhance the adversarial robustness of the learnt  1-Lipschitz models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857464-3ed3-436e-8853-9ed67d0ce5c6",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations\n",
    "You now know how to create 1-Lipschitz neural networks!\n",
    "\n",
    "In the next tutorial, we will see how to train and assess adversarially robust 1-Lipschitz neural networks on the classification task, using `deel-lip`'s `losses` submodule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
