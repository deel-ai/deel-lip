{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: HKR classifier on MNIST dataset\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deel-ai/deel-lip/blob/master/doc/notebooks/demo3.ipynb)\n",
    "\n",
    "This notebook will demonstrate learning a binary task on the MNIST0-8 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install deel-lip -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from deel.lip.layers import (\n",
    "    SpectralConv2D,\n",
    "    SpectralDense,\n",
    "    FrobeniusDense,\n",
    "    ScaledL2NormPooling2D,\n",
    ")\n",
    "from deel.lip.activations import MaxMin, GroupSort, GroupSort2, FullSort\n",
    "from deel.lip.losses import HKR, KR, HingeMargin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, wich is compatible\n",
    "with the Hinge term used in the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 11774 samples, classes proportions: 50.306 percent\n",
      "test set size: 1954 samples, classes proportions: 50.154 percent\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# first we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(x, y, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function convert the MNIST data to make it suitable for our binary classification\n",
    "    setup.\n",
    "    \"\"\"\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    x = x.astype(\"float32\")\n",
    "    y = y.astype(\"float32\")\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x /= 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "    y[y == class_a] = 1.0\n",
    "    y[y == class_b] = -1.0\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# now we load the dataset\n",
    "(x_train, y_train_ord), (x_test, y_test_ord) = mnist.load_data()\n",
    "\n",
    "# prepare the data\n",
    "x_train, y_train = prepare_data(\n",
    "    x_train, y_train_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "x_test, y_test = prepare_data(\n",
    "    x_test, y_test_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "\n",
    "# display infos about dataset\n",
    "print(\n",
    "    \"train set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (y_train.shape[0], 100 * y_train[y_train == 1].sum() / y_train.shape[0])\n",
    ")\n",
    "print(\n",
    "    \"test set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (y_test.shape[0], 100 * y_test[y_test == 1].sum() / y_test.shape[0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build lipschitz Model\n",
    "\n",
    "Let's first explicit the paremeters of this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# network parameters\n",
    "activation = GroupSort  # ReLU, MaxMin, GroupSort2\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 2.0\n",
    "alpha = 10.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the network.\n",
    "Here the experiment is done with a MLP. But `Deel-lip` also provide state of the art 1-Lipschitz convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 10:22:47.009179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.034880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.035148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.035589: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-19 10:22:47.036541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.036790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.037024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.368803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.369061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.369280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-19 10:22:47.369486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6556 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-07-19 10:22:47.931040: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lipModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " spectral_dense (SpectralDen  (None, 32)               50241     \n",
      " se)                                                             \n",
      "                                                                 \n",
      " spectral_dense_1 (SpectralD  (None, 16)               1057      \n",
      " ense)                                                           \n",
      "                                                                 \n",
      " frobenius_dense (FrobeniusD  (None, 1)                32        \n",
      " ense)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,330\n",
      "Trainable params: 25,664\n",
      "Non-trainable params: 25,666\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# helper function to build the 1-lipschitz MLP\n",
    "wass = Sequential(\n",
    "    layers=[\n",
    "        Input((28, 28, 1)),\n",
    "        Flatten(),\n",
    "        SpectralDense(32, GroupSort2(), use_bias=True),\n",
    "        SpectralDense(16, GroupSort2(), use_bias=True),\n",
    "        FrobeniusDense(1, activation=None, use_bias=False),\n",
    "    ],\n",
    "    name=\"lipModel\",\n",
    ")\n",
    "wass.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/franck.mamalet/.conda/envs/deel-tf2.8/lib/python3.10/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the output of our classifier is in the real range [-1, 1], binary accuracy must be redefined\n",
    "def HKR_binary_accuracy(y_true, y_pred):\n",
    "    S_true = tf.dtypes.cast(tf.greater_equal(y_true[:, 0], 0), dtype=tf.float32)\n",
    "    S_pred = tf.dtypes.cast(tf.greater_equal(y_pred[:, 0], 0), dtype=tf.float32)\n",
    "    return binary_accuracy(S_true, S_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wass.compile(\n",
    "    loss=HKR(\n",
    "        alpha=alpha, min_margin=min_margin\n",
    "    ),  # HKR stands for the hinge regularized KR loss\n",
    "    metrics=[\n",
    "        KR(),  # shows the KR term of the loss\n",
    "        HingeMargin(min_margin=min_margin),  # shows the hinge term of the loss\n",
    "        HKR_binary_accuracy,  # shows the classification accuracy\n",
    "    ],\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn classification on MNIST\n",
    "\n",
    "Now the model is build, we can learn the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 2s 10ms/step - loss: -3.2245 - KR: 4.4264 - HingeMargin: 0.1202 - HKR_binary_accuracy: 0.9507 - val_loss: -5.2971 - val_KR: 5.7282 - val_HingeMargin: 0.0449 - val_HKR_binary_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 0s 5ms/step - loss: -5.3499 - KR: 5.7587 - HingeMargin: 0.0409 - HKR_binary_accuracy: 0.9857 - val_loss: -5.4849 - val_KR: 5.8323 - val_HingeMargin: 0.0364 - val_HKR_binary_accuracy: 0.9835\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 0s 5ms/step - loss: -5.4647 - KR: 5.8403 - HingeMargin: 0.0376 - HKR_binary_accuracy: 0.9865 - val_loss: -5.4293 - val_KR: 5.9406 - val_HingeMargin: 0.0515 - val_HKR_binary_accuracy: 0.9796\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 0s 5ms/step - loss: -5.5039 - KR: 5.8894 - HingeMargin: 0.0385 - HKR_binary_accuracy: 0.9856 - val_loss: -5.5837 - val_KR: 5.9144 - val_HingeMargin: 0.0352 - val_HKR_binary_accuracy: 0.9850\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 1s 6ms/step - loss: -5.5247 - KR: 5.8956 - HingeMargin: 0.0371 - HKR_binary_accuracy: 0.9872 - val_loss: -5.5824 - val_KR: 5.9709 - val_HingeMargin: 0.0400 - val_HKR_binary_accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 1s 6ms/step - loss: -5.5415 - KR: 5.9200 - HingeMargin: 0.0378 - HKR_binary_accuracy: 0.9867 - val_loss: -5.5872 - val_KR: 5.9606 - val_HingeMargin: 0.0386 - val_HKR_binary_accuracy: 0.9835\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 1s 5ms/step - loss: -5.5678 - KR: 5.9335 - HingeMargin: 0.0366 - HKR_binary_accuracy: 0.9863 - val_loss: -5.6329 - val_KR: 5.9764 - val_HingeMargin: 0.0362 - val_HKR_binary_accuracy: 0.9864\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 1s 6ms/step - loss: -5.6128 - KR: 5.9675 - HingeMargin: 0.0355 - HKR_binary_accuracy: 0.9872 - val_loss: -5.6844 - val_KR: 6.0093 - val_HingeMargin: 0.0342 - val_HKR_binary_accuracy: 0.9864\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 1s 6ms/step - loss: -5.7151 - KR: 6.0544 - HingeMargin: 0.0339 - HKR_binary_accuracy: 0.9881 - val_loss: -5.8252 - val_KR: 6.1300 - val_HingeMargin: 0.0320 - val_HKR_binary_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 1s 6ms/step - loss: -5.8045 - KR: 6.1323 - HingeMargin: 0.0328 - HKR_binary_accuracy: 0.9885 - val_loss: -5.8965 - val_KR: 6.1319 - val_HingeMargin: 0.0260 - val_HKR_binary_accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1de81e6e00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wass.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model reach a very decent accuracy on this task."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e585d72a124540032141457729caea4129d351be49f1f69f41c00c4f8476abb5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
